{
    "spec_version": 5,
    "project": "dough",
    "sequence_length": 2048,
    "pages_per_window": 16,
    "batch_size": 6,
    "learning_rate": 2e-4,
    "blocks_per_window": 11,
    "windows_per_weights": 35,
    "momentum_decay": 0.999,
    "topk_compression": 32,
    "target_chunk": 64,
    "binary_score_ma_alpha": 0.05,
    "moving_average_window": 5,
    "tokenizer_name": "togethercomputer/LLaMA-2-7B-32K",
    "hidden_size": 4096,
    "num_hidden_layers": 32,
    "num_attention_heads": 32,
    "intermediate_size": 14336,
    "num_key_value_heads": 8,
    "activation_function": "silu",
    "max_position_embeddings": 2048,
    "weight_decay": 0.1,
    "warmup_steps": 250,
    "alpha_f": 0.1,
    "t_max": 20000,
    "validator_offset": 2,
    "checkpoint_frequency": 50,
    "max_topk_peers": 15,
    "minimum_peers": 5,
    "peer_replacement_frequency": 5,
    "peer_list_window_margin": 2,
    "active_check_interval": 60,
    "registration_check_interval": 600,
    "recent_windows": 5,
    "power_normalisation": 2.0,
    "validator_sample_rate": 0.6,
    "gather_peers_slash_threshold": 0.4,
    "uids_per_window": 7,
    "time_window_delta_seconds": 10,
    "reset_inactivity_windows": 25,
    "sync_max_steps_behind": 3,
    "eval_lr_factor": 0.2,
    "openskill_beta": 7,
    "openskill_tau": 0.1,
    "checkpoint_init_version": null,
    "num_evaluation_bins": 8,
    "quantization_bins": 256,
    "quantization_range": 6
}
